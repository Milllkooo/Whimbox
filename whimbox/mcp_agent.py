import asyncio
import requests
from langchain_mcp_adapters.client import MultiServerMCPClient
from langgraph.prebuilt import create_react_agent
from langchain.chat_models import init_chat_model
from langgraph.checkpoint.memory import MemorySaver
from whimbox.common.logger import logger
from whimbox.config.config import global_config
from whimbox.common.cvars import MCP_CONFIG


def is_mcp_ready(url: str) -> bool:
    try:
        response = requests.get(url)
        return response.status_code == 200
    except Exception as e:
        logger.error(f"æ£€æŸ¥MCPæœåŠ¡å™¨æ˜¯å¦å¥åº·å¤±è´¥: {e}")
        return False

class Agent:

    _instance = None
    _initialized = False

    def __new__(cls):
        """å•ä¾‹æ¨¡å¼"""
        if cls._instance is None:
            cls._instance = super(Agent, cls).__new__(cls)
        return cls._instance

    def __init__(self):
        if self._initialized:
            return
        self.langchain_agent = None
        self.err_msg = ""
        self.llm = None
        self.memory = None
        self.tools = None

        self._initialized = True

    async def start(self):
        logger.debug("å¼€å§‹åˆå§‹åŒ–agent")
        self.err_msg = "å‡†å¤‡ä¸­ï¼Œè¯·ç¨ç­‰..."
        api_key = global_config.get("Agent", "api_key")
        if not api_key:
            self.langchain_agent = None
            self.err_msg = "è¯·å…ˆå‰å¾€è®¾ç½®ï¼Œé…ç½®å¤§æ¨¡å‹çš„apiå¯†é’¥ã€‚ä¸ä¼šé…ç½®ï¼Ÿå¯åŠ¨å™¨å…¬å‘Šé‡Œæœ‰è¯¦ç»†ç™½å«–æ•™ç¨‹ï¼"
            self.llm = None
            logger.error(self.err_msg)
        else:
            try:
                self.llm = init_chat_model(
                    model=global_config.get("Agent", "model"),
                    model_provider=global_config.get("Agent", "model_provider"),
                    base_url=global_config.get("Agent", "base_url"),
                    api_key=api_key
                )
            except Exception as e:
                self.llm = None
                self.err_msg = f"AIåˆå§‹åŒ–å¤±è´¥ã€‚è¯·å‰å¾€è®¾ç½®ï¼Œæ£€æŸ¥å¤§æ¨¡å‹ç›¸å…³é…ç½®ã€‚"
                logger.error(self.err_msg)

        # åˆå§‹åŒ–mcp toolä¿¡æ¯(ä¸é‡å¤åˆå§‹åŒ–)
        if self.tools is None:
            server_url = f"http://127.0.0.1:{MCP_CONFIG['port']}"
            flag = False
            for _ in range(10):
                if is_mcp_ready(f'{server_url}/health'):
                    flag = True
                    break
                await asyncio.sleep(0.5)
            if flag:
                logger.debug("MCP server ready")
                client = MultiServerMCPClient({
                    "whimbox": {
                        "url": f'{server_url}/mcp',
                        "transport": "streamable_http",
                        "sse_read_timeout": MCP_CONFIG["timeout"],
                    }
                })
                self.tools = await client.get_tools()
            else:
                self.err_msg = "MCPæœªå°±ç»ªï¼Œè¯·é‡å¯å¥‡æƒ³ç›’"
                logger.error(self.err_msg)
        
        # åˆå§‹åŒ–memoryï¼ˆä¸é‡å¤åˆå§‹åŒ–ï¼‰
        if self.memory is None:
            self.memory = MemorySaver()
        
        if self.llm and self.tools and self.memory:
            self.langchain_agent = create_react_agent(
                model=self.llm, 
                tools=self.tools, 
                checkpointer=self.memory, 
                prompt=global_config.prompt, 
                debug=False)
            self.err_msg = ""
            logger.debug("MCP AGENT åˆå§‹åŒ–å®Œæˆ")
        else:
            self.langchain_agent = None
            logger.error("MCP AGENT åˆå§‹åŒ–å¤±è´¥")

    def is_ready(self):
        status = self.langchain_agent is not None
        return status, self.err_msg

    async def query_agent(self, text, thread_id="default", stream_callback=None, status_callback=None):
        logger.debug("å¼€å§‹è°ƒç”¨å¤§æ¨¡å‹")
        config = {"configurable": {"thread_id": thread_id}}
        input = {"messages": [{"role": "user", "content": text}]}
        
        full_response = ""
        
        # é€šçŸ¥å¼€å§‹æ€è€ƒ
        if status_callback:
            status_callback("thinking")
        
        async for event in self.langchain_agent.astream_events(input, config=config):
            # print(f"Event: {event.get('event')}")
            # if event.get('event') in ['on_tool_start', 'on_tool_end', 'on_tool_error']:
            #     print(f"Tool Event Details: {event}")
            # elif 'tool' in str(event.get('event', '')).lower():
            #     print(f"Unknown Tool Event: {event}")
            
            # å¤„ç†ä¸åŒç±»å‹çš„æµå¼äº‹ä»¶
            event_type = event.get("event")
            data = event.get("data", {})
            
            if event_type == "on_chat_model_stream":
                # å¤„ç†AIæ¨¡å‹çš„æµå¼è¾“å‡º
                chunk = data.get("chunk")
                if chunk and hasattr(chunk, 'content') and chunk.content:
                    content = chunk.content
                    full_response += content
                    if stream_callback and content.strip():  # åªå‘é€éç©ºå†…å®¹
                        stream_callback(content)
            
            elif event_type == "on_tool_start":
                # å·¥å…·è°ƒç”¨å¼€å§‹
                tool_name = event.get("name", "")
                if stream_callback:
                    stream_callback(f"ğŸ”§ ä»»åŠ¡è¿›è¡Œä¸­ï¼ŒæŒ‰â€œå¼•å·â€é”®ï¼Œéšæ—¶ç»ˆæ­¢ä»»åŠ¡\n")
                if status_callback:
                    status_callback("on_tool_start", tool_name)
            
            elif event_type == "on_tool_end":
                # å·¥å…·è°ƒç”¨ç»“æŸ
                tool_output = data.get("output", "")
                if stream_callback:
                    stream_callback(f"ğŸ’­ ä»»åŠ¡å®Œæˆï¼Œæ€»ç»“æˆæœä¸­~\n")
                if status_callback:
                    status_callback("on_tool_end", tool_name)
            
            elif event_type == "on_tool_error":
                # å·¥å…·è°ƒç”¨é”™è¯¯
                error = data.get("error", "")
                if stream_callback:
                    stream_callback(f"âŒ ä»»åŠ¡å¤±è´¥: {error}\n")
            
            elif event_type == "on_chat_model_start":
                # å¼€å§‹ç”Ÿæˆå›å¤
                if status_callback:
                    status_callback("generating")
            
            elif event_type == "on_chain_end":
                # æ•´ä¸ªé“¾æ¡ç»“æŸï¼Œè·å–æœ€ç»ˆç»“æœ
                output = data.get("output")
                if output and hasattr(output, 'content'):
                    # å¦‚æœæœ‰æœ€ç»ˆå†…å®¹ï¼Œç¡®ä¿åŒ…å«åœ¨å“åº”ä¸­
                    if output.content and output.content not in full_response:
                        final_content = output.content
                        full_response += final_content
                        if stream_callback:
                            stream_callback(final_content)
        
        logger.debug("å¤§æ¨¡å‹è°ƒç”¨å®Œæˆ")
        return full_response

    def get_ai_message(self, resp):
        ai_msgs = []
        for msg in resp['messages']:
            if msg.type == 'ai':
                ai_msgs.append(msg.content)
        return '\n'.join(ai_msgs)

mcp_agent = Agent()